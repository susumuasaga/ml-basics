{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat import v1\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow: Static Graphs\n",
    "-------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation uses basic TensorFlow operations to set up a computational\n",
    "graph, then executes the graph many times to actually train the network.\n",
    "\n",
    "One of the main differences between TensorFlow and PyTorch is that TensorFlow\n",
    "uses static computational graphs while PyTorch uses dynamic computational\n",
    "graphs.\n",
    "\n",
    "In TensorFlow we first set up the computational graph, then execute the same\n",
    "graph many times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Semantix\\ml-basics\\conda-env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "99 405.4677\n",
      "199 1.955998\n",
      "299 0.026078273\n",
      "399 0.0007728277\n",
      "499 0.000104765175\n"
     ]
    }
   ],
   "source": [
    "v1.disable_eager_execution()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N = 64\n",
    "D_in = 1000\n",
    "H = 100\n",
    "D_out = 10\n",
    "\n",
    "x = v1.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = v1.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "w1 = tf.Variable(tf.random.normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random.normal((H, D_out)))\n",
    "\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(y - y_pred))\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, (w1, w2))\n",
    "\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "with v1.Session() as sess:\n",
    "    sess.run(v1.global_variables_initializer())\n",
    "\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for t in range(500):\n",
    "        loss_value, _, _ = sess.run(\n",
    "            [loss, new_w1, new_w2], feed_dict={x: x_value, y: y_value}\n",
    "        )\n",
    "        if t % 100 == 99:\n",
    "            print(t, loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}